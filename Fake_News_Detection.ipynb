{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPC4AGHwNUOWSPXRZHmOWE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tekliyetamiru/Fake-News-Detections-By-Using-DistilBERT-MobileBERT-and-TinyBERT-Model/blob/main/Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tuning DistilBERT, MobileBERT and TinyBERT for Fake News Detection**"
      ],
      "metadata": {
        "id": "nZkXti0CvIb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U transformers\n",
        "# !pip install -U accelerate\n",
        "# !pip install -U datasets\n",
        "# !pip install -U bertviz\n",
        "# !pip install -U Umap-learn\n",
        "# !pip install seaborn --upgrade\n",
        "\n",
        "# !pip install -U openpyxl\n",
        "\n",
        "# Don't do in production. Doing now to keep output clean for understanding\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "6qa122I00xv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading**"
      ],
      "metadata": {
        "id": "wGKe3Mr4yige"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "datafile = pd.read_excel(\"https://github.com/tekliyetamiru/Fake-News-Detections-By-Using-DistilBERT-MobileBERT-and-TinyBERT-Model/raw/main/fake_news.xlsx\")\n",
        "datafile.head()"
      ],
      "metadata": {
        "id": "s9yYwrR5x0pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafile.isnull().sum()"
      ],
      "metadata": {
        "id": "gjNNmtaqONbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafile = datafile.dropna()\n",
        "datafile.isnull().sum()"
      ],
      "metadata": {
        "id": "hOeSZoNnOqJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafile['label'].value_counts()"
      ],
      "metadata": {
        "id": "n6rlZ3rNPCJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Analysis**"
      ],
      "metadata": {
        "id": "zHAZHVflPgmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "6ZQ7UP86Potk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = datafile['label'].value_counts(ascending=True)\n",
        "label_counts.plot.barh()\n",
        "plt.title(\"Frequency of Classes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhtGKszUPz3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.5 tokens per word on average\n",
        "\n",
        "datafile['title_tokens'] = datafile['title'].apply(lambda x: len(x.split())*1.5)\n",
        "datafile['text_tokens'] = datafile['text'].apply(lambda x: len(x.split())*1.5)\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "ax[0].hist(datafile['title_tokens'], bins=50, color=\"skyblue\")\n",
        "ax[0].set_title(\"Title Tokens\")\n",
        "\n",
        "ax[1].hist(datafile['text_tokens'], bins=50, color=\"orange\")\n",
        "ax[1].set_title(\"Text Tokens\")\n"
      ],
      "metadata": {
        "id": "DIHJxeulQkMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split Dataset into train and test**"
      ],
      "metadata": {
        "id": "SSTTVeAmCxtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 70% for traing, 20% for test and 10% for validation\n",
        "train, test = train_test_split(datafile, test_size = 0.3, stratify=datafile['label'])\n",
        "test, validation = train_test_split(test, test_size=1/3, stratify=test['label'])\n",
        "\n",
        "datafile.shape,train.shape, test.shape, validation.shape"
      ],
      "metadata": {
        "id": "e_MCOSQlDGWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "dataset= DatasetDict({\n",
        "    \"train\":Dataset.from_pandas(train, preserve_index=False),\n",
        "    \"test\": Dataset.from_pandas(test, preserve_index=False),\n",
        "    \"validation\":Dataset.from_pandas(validation, preserve_index=False)\n",
        "})"
      ],
      "metadata": {
        "id": "MvrJvfYfK9ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "wTkDqFNwQDxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Tokenization**"
      ],
      "metadata": {
        "id": "5FcNaBgXQkNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "text = \"machine learning is awesome!! Thanks KG take.\"\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "distilbert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "distilbert_tokens = distilbert_tokenizer.tokenize(text)\n",
        "\n",
        "model_ckpt = \"google/mobilebert-uncased\"\n",
        "mobilebert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "mobilebert_tokens = mobilebert_tokenizer.tokenize(text)\n",
        "\n",
        "model_ckpt = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "tinybert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "tinybert_tokens = tinybert_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "59FX_jCjQuog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_tokenizer, mobilebert_tokenizer, tinybert_tokenizer"
      ],
      "metadata": {
        "id": "4sVxPmUmuOI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  temp = distilbert_tokenizer(batch['title'],padding=True, truncation=True)\n",
        "  return temp\n",
        "\n",
        "print(tokenize(dataset['train'][:2]))"
      ],
      "metadata": {
        "id": "_07lsRFgvx9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = dataset.map(tokenize, batch_size=None, batched=True)"
      ],
      "metadata": {
        "id": "8T5-eGS-xryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Building**"
      ],
      "metadata": {
        "id": "OtXaIEiA0EXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification,AutoConfig\n",
        "import torch\n",
        "\n",
        "label2id = {\"Real\":0,\"Fake\":1}\n",
        "id2label = {0:\"Real\",1:\"Fake\"}\n",
        "\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "# model_ckpt = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "# model_ckpt = \"google/mobilebert-uncased\"\n",
        "\n",
        "num_labels = len(label2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_ckpt,label2id=label2id, id2label=id2label)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,config=config).to(device)"
      ],
      "metadata": {
        "id": "ENCqZTuh0KIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "nXDdrcdl7c9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "J4BiG2Z88vVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "ZHo5NWCeAfk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build compute metrics function\n",
        "# !pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics_evaluate(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  return accuracy.compute(predictions=predictions,references=labels)"
      ],
      "metadata": {
        "id": "bGvRVUMA_hVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 32\n",
        "training_dir = \"train_dir\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=training_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy='epoch',\n",
        "    disable_tqdm=False\n",
        ")"
      ],
      "metadata": {
        "id": "psYFVi8P1X57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args"
      ],
      "metadata": {
        "id": "2kY-fiEc1X3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    compute_metrics=compute_metrics_evaluate,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    tokenizer=distilbert_tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "tNDteiYlCj1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "mhOGedHaDaAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**"
      ],
      "metadata": {
        "id": "wPocc0EeMQdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output=trainer.predict(encoded_dataset['test'])"
      ],
      "metadata": {
        "id": "Ks5aGcfCMV4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output.metrics"
      ],
      "metadata": {
        "id": "IzdpXgFDMjnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(preds_output.predictions,axis=1)\n",
        "y_true = encoded_dataset['test'][:]['label']"
      ],
      "metadata": {
        "id": "z5SnnQ3gM5TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred, target_names=list(label2id)))"
      ],
      "metadata": {
        "id": "qplGv0uQNeuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benchmarking**"
      ],
      "metadata": {
        "id": "EVSE--wGOHXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sklearn to build compute metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  f1 = f1_score(labels,preds,average=\"weighted\")\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\"accuracy\":acc, \"f1\":f1}"
      ],
      "metadata": {
        "id": "Zoc8kAasONuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_dict={\n",
        "    \"bert-base\":\"bert-base-uncased\",\n",
        "    \"distilbert\":\"distilbert-base-uncased\",\n",
        "    \"mobilebert\":\"google/mobilebert-uncased\",\n",
        "    \"tinybert\":\"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "}\n",
        "\n",
        "def train_model(model_name):\n",
        "  model_ckpt=model_dict[model_name]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "  config = AutoConfig.from_pretrained(model_ckpt,label2id=label2id, id2label=id2label)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,config=config).to(device)\n",
        "\n",
        "  def local_tokenizer(batch):\n",
        "    temp = tokenizer(batch['title'],padding=True,truncation=True)\n",
        "    return temp\n",
        "\n",
        "  encoded_dataset=dataset.map(local_tokenizer,batched=True,batch_size=None)\n",
        "\n",
        "  trainer = Trainer(\n",
        "    model=model,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    tokenizer=tokenizer\n",
        "    )\n",
        "  trainer.train()\n",
        "\n",
        "  preds = trainer.predict(encoded_dataset['test'])\n",
        "\n",
        "  return preds.metrics\n",
        "\n",
        "import time\n",
        "model_performance={}\n",
        "for model_name in model_dict:\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"Training Model: \", model_name)\n",
        "\n",
        "  start = time.time()\n",
        "  result = train_model(model_name)\n",
        "  end = time.time()\n",
        "  model_performance[model_name] = {model_name:result,\"time taken\":end-start}"
      ],
      "metadata": {
        "id": "eQmjtr1oP1Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_performance"
      ],
      "metadata": {
        "id": "bZMg3ux7ip9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}